---
title: "TextMining - Discursos Presidenciales tidy way"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r libraries, message=FALSE, warning=FALSE}
library (tidyverse)
library (tidytext)
library (ggrepel)
library (tm)
```
```{r read_data, message=FALSE, warning=FALSE}
filenames <- list.files(".",pattern="*.txt")
discursos_raw = map(filenames,~readLines(.x,encoding="UTF-8")) %>% 
  map2(filenames, ~tibble(text=.x, docid=.y)) %>% 
  map_df(rbind) #bind_rows()

# Replaces varios
discursos_raw = discursos_raw %>% 
  filter(text != "") %>% 
  mutate(text = gsub("SS.SS", "Sus Señorías", text)
         ,text = gsub("SS. SS", "Sus Señorías", text)
         ,text = gsub("s.s s.s", "Sus Señorías", text)
         )
discursos_raw 
```

```{r tyding, message=FALSE, warning=FALSE}
discursos = discursos_raw %>%
  group_by(docid) %>%
  mutate(linenumber = row_number()) %>% 
  ungroup() %>% 
  unnest_tokens(word, text) 

add.stops=c(
  "vez", "sino", "cada", "ello", "así", "sólo", "digo", 
  "que", "que,", "señorías","gobierno",
  "españa","españoles","país","ser","hacia","años",
  "debe","cualquier","año","manera","todas","mayor",
  "parte","presidenta","ustedes","vista","señora","hecho","sus")
#add.stops=c("kk")
stop_words = append(add.stops, tm::stopwords("spanish"))

discursos = discursos %>% 
  filter(!word %in% stop_words) 

discursos_tfidf = discursos %>% 
  count(docid, word, sort = TRUE) %>%
  ungroup() %>% 
  bind_tf_idf(word, docid, n) %>% 
  arrange(desc(tf_idf))

discursos_tfidf %>% arrange(desc(n))
```
```{r plots words, message=FALSE, warning=FALSE}
discursos_tfidf %>% 
  group_by(docid) %>% 
  top_n(3) %>% 
  ungroup() %>% 
  arrange(docid) %>% 
  ggplot(aes(word,tf_idf, color=docid)) + 
    geom_label_repel(aes(label=word)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    theme(legend.position="bottom")
```
```{r td-idf words by doc, message=FALSE, warning=FALSE}
plots <- discursos_tfidf %>%
  group_by(docid) %>% 
  top_n(5) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) %>%
  split(.$docid) %>%
  map(~ ggplot(.x, aes(word,tf_idf)) + 
          geom_bar(stat="identity", aes(fill=docid)) + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          theme(legend.position="bottom")
  )

walk(plots, print)
```

```{r tidy bigrams}
discursos_bigram_raw = discursos_raw %>%     
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

add.bigramsstops=c("señora presidenta", "sus señorías")
discursos_bigram_raw = discursos_bigram_raw %>% 
  filter(!bigram %in% add.bigramsstops)
  
discursos_bigram = discursos_bigram_raw %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words) %>%
  filter(!word2 %in% stop_words) %>% 
  unite(bigram, word1, word2, sep = " ") 

discursos_bigram_tfidf = discursos_bigram %>% 
  count(docid, bigram, sort = TRUE) %>%
  ungroup() %>% 
  bind_tf_idf(bigram, docid, n) %>% 
  arrange(desc(tf_idf))

discursos_bigram
```
```{r plots bigrams, message=FALSE, warning=FALSE}
discursos_bigram_tfidf %>% 
  group_by(docid) %>% 
  top_n(3) %>% 
  ungroup() %>% 
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram,tf_idf, color=docid)) + 
    geom_label_repel(aes(label=bigram)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    theme(legend.position="bottom")
```
```{r td-idf bigrams by doc, message=FALSE, warning=FALSE}
plots <- discursos_bigram_tfidf %>%
  group_by(docid) %>% 
  top_n(5) %>% 
  ungroup() %>% 
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  split(.$docid) %>%
  map(~ ggplot(.x, aes(bigram,tf_idf)) + 
          geom_bar(stat="identity", aes(fill=docid)) + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          theme(legend.position="bottom")
  )

walk(plots, print)
```

```{r, message=FALSE, warning=FALSE}
require(quanteda)

pcorpus = corpus(discursos_raw%>% 
                   split(.$docid) %>% 
                   map(~ paste0(.x, collapse = " ")) %>% 
                   map_chr(~ paste0(.x,collapse= " ")))
options(width = 200)
kwic(pcorpus, "iraq")

```

```{r, message=FALSE, warning=FALSE}
dfm = discursos_tfidf %>% select(docid, word, n) %>% cast_dfm(docid, word, n)
topfeatures(dfm, 20)
```
```{r, message=FALSE, warning=FALSE}

filterwords = c("constitución", "empleo", "crisis", "españa","desempleo","paro","nación","pais","cataluña","trabajo","eta","vasco","pacto","autónomas","terrorismo","terrorista","terroristas","europa","europea","corrupción", "corrupto","gal","atentado", "atentados","droga","drogas")

pscale = discursos_tfidf %>% 
  group_by(docid) %>%
  mutate(docid_total = sum(n)) %>% 
  ungroup() %>% 
  filter(word %in% filterwords) %>%
  ggplot(aes(docid, n/docid_total)) +
    geom_bar(stat="identity", aes(fill=docid)) +
    geom_smooth() +
    facet_wrap(~ word, scales = "free_y") +
    scale_y_continuous(labels = scales::percent_format()) +
    ylab("% frequency of word in inaugural address") +
      theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank()) +
      theme(legend.position="bottom"
            ,legend.direction="horizontal"
            ,legend.key.width = unit(1, "mm") ) +
      theme(legend.position = 'none')

pnoscale = discursos_tfidf %>% 
  group_by(docid) %>%
  mutate(docid_total = sum(n)) %>% 
  ungroup() %>% 
  filter(word %in% filterwords) %>%
  ggplot(aes(docid, n/docid_total)) +
    geom_bar(stat="identity", aes(fill=docid)) +
    geom_smooth() +
    facet_wrap(~ word) +
    scale_y_continuous(labels = scales::percent_format()) +
    ylab("% frequency of word in inaugural address") +
      theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank()) +
      theme(legend.position="bottom"
            ,legend.direction="horizontal"
            ,legend.key.width = unit(1, "mm") ) +
      theme(legend.position = 'none')

pscale
pnoscale


```
```{r terrorismo, message=FALSE, warning=FALSE}

terrorismo_keywords = c("atentad*","terror*","eta","gal","yihad*","muertos")

#options(width = 400)
#kwic(pcorpus, terrorismo_keywords,window = 15)
#quanteda::textstat_simil (dfm, "00.txt", margin = "documents")
relacion_docs = as.matrix(quanteda::textstat_simil (dfm, margin = "documents", upper=TRUE, diag=TRUE))

terrorismo_terms = as.data.frame(kwic(pcorpus, terrorismo_keywords,window = 10)) %>% select(keyword) %>% mutate(keyword=char_tolower(keyword)) %>% unique() 

library (corrplot)

corrplot(relacion_docs, method = "ellipse", type="lower", order = "alphabet")
corrplot(relacion_docs, method = "pie", type="lower", order = "alphabet")
corrplot(relacion_docs, method = "pie", type="lower", order = "FPC")
corrplot(relacion_docs, method = "pie", type="lower", order = "AOE")
corrplot(relacion_docs, method = "number", type="lower", order = "hclust")

library(ggcorrplot)
ggcorrplot(relacion_docs, method = "circle", lab=T, lab_size = 3, type="lower", hc.order=T, ggtheme=theme_classic)


```

```{r}
#library(treemapify)
```

```{r terms correlation, message=FALSE, warning=FALSE}

#options(width = 400)
#kwic(pcorpus, terrorismo_keywords,window = 15)
#quanteda::textstat_simil (dfm, "00.txt", margin = "documents")
#relacion_words = as.matrix(quanteda::textstat_simil (dfm, margin = "features", upper=TRUE, diag=TRUE))


#corrplot(relacion_words, method = "ellipse", type="lower", order = "alphabet")
#como hablan del terrorismo
discursos %>%
  filter(word %in% terrorismo_terms$keyword) %>%
  ggplot(aes(docid, linenumber)) +
    geom_point (aes(fill=word, color=word ) ) +
    geom_smooth() +
    geom_text_repel(aes(label=word, color=word)) +
    coord_flip() + 
    facet_grid(docid~., scales = "free")

#como hablan del desempleo
discursos %>%
   filter(word %in% c("empleo","desempleo")) %>%
   ggplot(aes(docid, linenumber)) +
     geom_point (aes(fill=word, color=word, shape=word, size=5, alpha=1/4 )) +
     geom_smooth() +
     coord_flip() + 
     facet_grid(docid~., scales = "free")

#lo mismo pintado sin facets
discursos %>% 
    filter(word %in% c("empleo","desempleo")) %>%
    ggplot(aes(docid, linenumber)) +
      geom_count (show.legend = F, aes( color=word, alpha=1/4, shape=word)) +
 coord_flip() 
#ggMarginal(p, type = "histogram", fill="transparent")

```
```{r ngrams raw}

discursos_ngrams = lapply(1:5, function(ngram_n) {
    discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>% 
    ungroup() %>% 
    unnest_tokens(output = word, input = text, token = "ngrams", n = ngram_n) %>% 
    mutate(ngram_n = ngram_n)
 }) %>% bind_rows()

# discursos_ngrams =
#     discursos_raw %>%
#     group_by(docid) %>%
#     mutate(linenumber = row_number()) %>% 
#     ungroup() %>% 
#     unnest_tokens(output = word, input = text, token = "regex") %>% 
#     mutate(word1 = word) %>%
#     mutate(word2 = lead(word, 1)) %>% 
#     mutate(word3 = lead(word, 2)) %>% 
#     mutate(word4 = lead(word, 3)) %>% 
#     mutate(word5 = lead(word, 4)) %>% 
#     filter(!word %in% stop_words$word, # remove stop words
#          !word2 %in% stop_words$word, # remove stop words
#          substr(word, 1, 1) != '#', # remove hashtags
#          str_detect(word, "[a-z]"), # remove words containing ony numbers or symbols
#          str_detect(word, "[a-z]")) %>% # remove words containing ony numbers or symbols
#     mutate(bigram = paste(word1, word2, sep = ' ') )


```
```{r ngrams}
vector.is.empty <- function(x) return(length(x) == 0 )
terms.in.list <- function(listain, lista) return( !vector.is.empty(intersect(unique(listain),lista)) )

# discursos_ngrams = discursos_ngrams %>% 
#   filter(!terms.in.list(unlist(strsplit(word, " ", fixed = T)), stop_words))
#   

discursos_ngrams_dfidf = discursos_ngrams %>%
     count(docid, word, sort = TRUE) %>%
     ungroup() %>%
     bind_tf_idf(term_col = word, document_col = docid, n_col = n)



```
```{r ngrams plot, message=FALSE, warning=FALSE}
discursos_ngrams_dfidf %>% 
  group_by(docid) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) %>%
  split(.$docid) %>%
  map(~ ggplot(.x, aes(word,tf_idf)) + 
          geom_bar(stat="identity", aes(fill=docid)) + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          theme(legend.position="bottom")
  )



```
```{r ngram by parrafo}
discursos_ngrams_parrafos_dfidf = discursos_ngrams %>%
    count(docid,linenumber, word, sort = TRUE) %>%
    ungroup() %>% 
    split(c(.$docid)) %>%
    map(~ .x %>% 
     bind_tf_idf(term_col = word, document_col = linenumber, n_col = n)
    ) %>% bind_rows()
```
```{r ngram by parrafo plot, message=FALSE, warning=FALSE}
discursos_ngrams_parrafos_dfidf %>% 
 filter(docid == "00.txt") %>% 
  group_by(linenumber) %>%
  top_n(3,tf_idf) %>% 
  ungroup() %>% 
  mutate(linenumber = reorder(linenumber, tf-idf)) %>%
 ggplot(aes(word,linenumber)) +
 geom_count (show.legend = F, aes(alpha=1/4)) +
  coord_flip() 

```



```{r}

add.stops=c(
  "vez", "sino", "cada", "ello", "así", "sólo", "digo", 
  "que", "que,", "señorías", "señoría","señora","gobierno",
  "españa","españoles","país","ser","hacia","años",
  "debe","cualquier","año","manera","todas","mayor",
  "parte","presidenta","ustedes","vista","señora","hecho","sus", "señor", "diputados", "presidente")
stop_words = append(add.stops, tm::stopwords("spanish"))
#stop_words = c("kk")


discursos_words = discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(output = word1, input = text, token = "words") %>%
    mutate(ngram_n = 1) %>% 
    filter(
      !word1 %in% stop_words, # remove stop words
      substr(word1, 1, 1) != '#' # remove hashtags
    ) %>% 
    rename(ngrams= word1)
    #mutate(bigram = paste(word1, word2, sep = ' ') )

discursos_bigrams =
    discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(output = word1, input = text, token = "words") %>%
    mutate(word2 = lead(word1, 1)) %>%
    mutate(ngram_n = 2) %>% 
    filter(
      !word1 %in% stop_words, # remove stop words
      !word2 %in% stop_words, # remove stop words
      !is.na(word2), # remove if NA
      substr(word1, 1, 1) != '#' # remove hashtags
    ) %>% 
    unite(ngrams, word1, word2,  sep = " ", remove=TRUE)
    #mutate(bigram = paste(word1, word2, sep = ' ') )

discursos_trigrams =
    discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(output = word1, input = text, token = "words") %>%
    mutate(word2 = lead(word1, 1)) %>%
    mutate(word3 = lead(word1, 2)) %>%
    mutate(ngram_n = 3) %>% 
    filter(
      !word1 %in% stop_words, # remove stop words
      !word2 %in% stop_words, # remove stop words
      !word3 %in% stop_words, # remove stop words
      !is.na(word3), # remove if NA
      substr(word1, 1, 1) != '#' # remove hashtags
    ) %>% 
    unite(ngrams, c(word1, word2, word3),  sep = " ", remove=TRUE)


discursos_n4grams =
    discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(output = word1, input = text, token = "words") %>%
    mutate(word2 = lead(word1, 1)) %>%
    mutate(word3 = lead(word1, 2)) %>%
    mutate(word4 = lead(word1, 3)) %>%
    mutate(ngram_n = 4) %>% 
    filter(
      !word1 %in% stop_words, # remove stop words
      !word2 %in% stop_words, # remove stop words
      !word3 %in% stop_words, # remove stop words
      !word4 %in% stop_words, # remove stop words
      !is.na(word4), # remove if NA
      substr(word1, 1, 1) != '#' # remove hashtags
    ) %>% 
    unite(ngrams, c(word1, word2, word3, word4),  sep = " ", remove=TRUE)

discursos_n5grams =
    discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(output = word1, input = text, token = "words") %>%
    mutate(word2 = lead(word1, 1)) %>%
    mutate(word3 = lead(word1, 2)) %>%
    mutate(word4 = lead(word1, 3)) %>%
    mutate(word5 = lead(word1, 4)) %>%
    mutate(ngram_n = 5) %>% 
    filter(
      !word1 %in% stop_words, # remove stop words
      !word2 %in% stop_words, # remove stop words
      !word3 %in% stop_words, # remove stop words
      !word4 %in% stop_words, # remove stop words
      !word5 %in% stop_words, # remove stop words
      !is.na(word5), # remove if NA
      substr(word1, 1, 1) != '#' # remove hashtags
    ) %>% 
    unite(ngrams, c(word1, word2, word3, word4, word5),  sep = " ", remove=TRUE)


discursos_ngrams2 = bind_rows(discursos_words ,discursos_bigrams, discursos_trigrams, discursos_n4grams, discursos_n5grams)

discursos_ngrams_dfidf2 = discursos_ngrams2 %>%
     count(docid, ngrams, sort = TRUE) %>%
     ungroup() %>%
     bind_tf_idf(term_col = ngrams, document_col = docid, n_col = n)


```

```{r ngrams2 plot tops by docid, message=FALSE, warning=FALSE}

discursos_ngrams_dfidf2 %>% 
  group_by(docid) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(ngrams = reorder(ngrams, tf_idf)) %>%
  split(.$docid) %>%
  map(~ ggplot(.x, aes(ngrams,tf_idf)) + 
          geom_bar(stat="identity", aes(fill=docid)) + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          theme(legend.position="bottom")
          
  )



```
```{r ngrams2 plot tops total, message=FALSE, warning=FALSE}

discursos_ngrams_dfidf2 %>% 
  top_n(20) %>% 
  mutate(ngrams = reorder(ngrams, tf_idf)) %>%
  split(.$docid) %>%
  map(~ ggplot(.x, aes(ngrams,tf_idf)) + 
          geom_bar(stat="identity", aes(fill=docid)) + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          theme(legend.position="bottom")
          
  )


```

```{r}

discursos_tfidf_by_ngrams_top = discursos_ngrams2 %>% 
  count(docid,ngrams,ngram_n, sort=T) %>% 
  ungroup() %>% 
  filter (n>1) %>% 
  split(.$ngram_n) %>% 
  map(~ .x %>% 
    filter(ngram_n == ngram_n) %>% 
    bind_tf_idf(term_col = ngrams, document_col = docid, n_col = n) 
  ) %>% bind_rows()

discursos_tfidf_by_ngrams_top %>% 
    mutate(ngrams = reorder(ngrams, tf_idf)) %>%
    group_by(docid) %>%
    top_n(10,tf_idf) %>% 
    ungroup() %>% 
    split(.$docid) %>%
    map(~ ggplot(.x, aes(ngrams,tf_idf)) + 
            geom_bar(stat="identity", aes(fill=docid)) + 
            theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
            theme(legend.position="bottom")
    ) 

# discursos_ngrams2 %>%
#     filter(ngram_n == 4) %>% 
#      count(docid, ngrams, sort = TRUE) %>%
#      ungroup() %>%
#      bind_tf_idf(term_col = ngrams, document_col = docid, n_col = n) %>% 
#     group_by(docid) %>%
#     top_n(1) %>% 
#     ungroup() %>% 
#     mutate(ngrams = reorder(ngrams, tf_idf)) %>%
#     split(.$docid) %>%
#     map(~ ggplot(.x, aes(ngrams,tf_idf)) + 
#             geom_bar(stat="identity", aes(fill=docid)) + 
#             theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
#             theme(legend.position="bottom")
#     )  

```
```{r}
# otra forma de filtrar los stop_words con gather y spread
discursos_raw %>%
    group_by(docid) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    unnest_tokens(output = word1, input = text, token = "words") %>%
    mutate(word2 = lead(word1, 1)) %>%
    mutate(word3 = lead(word1, 2)) %>%
    mutate(word4 = lead(word1, 3)) %>%
    mutate(ngram_n = 4) %>% 
	gather(name, value, -c(docid,linenumber,ngram_n) )%>% 
	filter(!value %in% stop_words) %>%  
	group_by(docid,linenumber,ngram_n) %>%  
	mutate(ind = row_number()) %>%
	spread(name,value) %>% select(-ind)
```
```{r lda tunning}
dtm = discursos_ngrams_dfidf2 %>% filter(n>1) %>% select(docid, ngrams, n) %>% cast_dtm(docid, ngrams, n)

library("ldatuning")
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(result)

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
```
```{r lda modeling}
#Number of topics
k <- 11

ldaOut5 <-LDA(dtm2, k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))

ldaOut5.topics <- as.matrix(topics(ldaOut5))
ldaOut5.terms <- as.matrix(terms(ldaOut5,10))
topicProbabilities <- as.data.frame(ldaOut5@gamma)

ldaOut5.terms
ldaOut5.topics
topicProbabilities

ggcorrplot(topicProbabilities, method = "circle", lab=T, lab_size = 3, type="lower", hc.order=F, ggtheme=theme_classic)

```

```{r, engine='python'}

import numpy as np

import matplotlib.pyplot as plt 
import pandas as pd

x=np.arange(-10,10)
print(x)

```


