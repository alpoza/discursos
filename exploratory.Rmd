---
title: "Discursos Presidenciales"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## GitHub Documents

This is an R Markdown format used for publishing markdown documents to GitHub. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.

## Including Code

You can include R code in the document as follows:

```{r librarys, include=FALSE}
library(tm)
library(wordcloud)
library(plyr)
library(tidyverse)
library(tidytext)
library(SnowballC)
library(ggrepel)
library(stringr)
```
```{r}
folderdir="data/"
speeches_raw = VCorpus(DirSource(folderdir, encoding = "UTF-8"), readerControl=list(reader=readPlain,language="es"))
#speeches<-Corpus(DirSource("data/"))
add.stops=c("vez", "sino", "cada", "ello", "así", "sólo", "que", "que,", "señorías", "señorías,","gobierno","política","españa","españoles","país","ser","hacia","años","debe","cualquier","año","manera","todas","mayor","parte","presidenta","ustedes","vista","señora")
speeches = speeches_raw
speeches = tm_map (speeches, content_transformer(tolower))
speeches = tm_map (speeches, removePunctuation)
speeches = tm_map (speeches, stripWhitespace)
speeches = tm_map (speeches, removeWords, stopwords("spanish"))
speeches = tm_map (speeches, removeWords, add.stops)
#speeches = tm_map (speeches, stemDocument, language="spanish")

my_scanner <- function(x) strsplit(x," ")
my_tokenizer <- function (x) 
{
  strsplit(iconv(x, to='UTF-8'), split='([[:space:]]|[[:punct:]])+', perl=F)[[1]]
}
speech.control=list(stopwords=c(stopwords("spanish"),add.stops), removeNumbers=FALSE, removePunctuation=FALSE) #, tokenize=my_tokenizer
speeches.dtm<-TermDocumentMatrix(speeches, control=speech.control)
notSparse = removeSparseTerms(speeches.dtm, 0.99)
tidy_speeches = tidy(speeches.dtm) %>% bind_tf_idf(term, document, count)

tidy_notSparse = tidy(notSparse)
speeches.matrix <- as.matrix(speeches.dtm)
findFreqTerms(speeches.dtm, lowfreq=100)
```
```{r, message=FALSE, warning=FALSE}
tidy_speeches %>% group_by(document) %>% top_n(10) %>% ungroup() %>% ggplot(aes(term,count)) + geom_bar(stat="identity", aes(colour=document, alpha=1/2)) + facet_wrap(~document, scales = "free", ncol = 1) + theme(legend.position="none")
```
```{r, message=FALSE, warning=FALSE}
tidy_speeches = tidy(speeches.dtm) %>% bind_tf_idf(term, document, count)

plot_speeches <- tidy_speeches %>%
   arrange(desc(tf_idf)) %>%
   mutate(word = factor(term, levels = rev(unique(term)))) 
#%>%  mutate(id = str_extract(document, "^.."))

plot_speeches %>% 
   top_n(50) %>%
   ggplot(aes(word, tf_idf, fill = document)) +
   geom_col() +
   geom_label_repel(aes(label=document)) +
   labs(x = NULL, y = "tf-idf") +
   coord_flip() +
  theme_classic(base_size = 16)
```

```{r }
m <- as.matrix(speeches.dtm)
v <- sort(rowSums(m),decreasing=TRUE)
df <- data.frame(word = names(v),freq=v)
```


```{r , echo=FALSE, warning=FALSE}
wordcloud(df$word,df$freq,min.freq=50)
```
```{r}
distancias <- as.matrix(dist(speeches.matrix, 
                             method = "binary", 
                             diag = TRUE, 
                             upper = FALSE, 
                             p = 2))
```

```{r corpus}
txt <- readLines("./00.txt",encoding="UTF-8")
#txt = iconv(txt, to="ASCII//TRANSLIT")
#corpus <- Corpus(VectorSource(txt))
txt00 = readLines("00.txt",encoding="UTF-8") %>% tibble()
txt00 = txt00 %>% filter(txt != "")
#txt = iconv(txt, to="ASCII//TRANSLIT")
corpus <- Corpus(VectorSource(txt00))
d  <- tm_map(corpus, tolower)
d  <- tm_map(d, stripWhitespace)
d <- tm_map(d, removePunctuation)
d <- tm_map(d, removeWords, stopwords("spanish"))
scanner <- function(x) strsplit(x," ")
tdm <- TermDocumentMatrix(d,control=list(tokenize=scanner))

findFreqTerms(tdm, lowfreq=20)

```

## Including Plots

You can also embed plots, for example:
```{r dataframe}
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
df <- data.frame(word = names(v),freq=v)
```


```{r wordcloud, echo=FALSE, warning=FALSE}
wordcloud(df$word,df$freq,min.freq=6)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r, message=FALSE, warning=FALSE}

names(txt00) = c('txt')
tidy_txt00 <- txt00 %>% mutate(cap='00') %>% 
  unnest_tokens(word, txt)

stop_words = as.data.frame(stopwords("spanish"))
names(stop_words) = c("word")
clean_txt00 = tidy_txt00 %>% 
  anti_join(stop_words)
clean_txt00 %>% count(word, sort = TRUE) 

```

```{r, message=FALSE, warning=FALSE}
paths <- dir(".", pattern = "\\01.txt$", full.names = TRUE)
lineas = ldply(paths, read.csv, stringsAsFactors = FALSE, header=T, sep="\t", encoding = 'UTF-8') %>% gather(na.rm = TRUE)
palabras = lineas %>% unnest_tokens(word,value) %>% anti_join(stop_words)
palabras %>% ggplot(aes(word, color=key)) + geom_bar(stat="count", aes(color=key)) + facet_grid(.~key)
```
```{r clusters, message=FALSE, warning=FALSE}
# Compute distances
distances = dist(df, method = "euclidean")
# Hierarchical clustering
clusterMovies = hclust(distances, method = "ward") 

# Plot the dendrogram
plot(clusterMovies)
```

```{r}
# Assign points to clusters
KMC = kmeans(df, centers = 3, iter.max = 1000)
wordsClusters = KMC$cluster
```
```{r topicsmodel LDA}
#from: https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/
require(topicmodels)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k = 5
dtm <- DocumentTermMatrix(speeches)
ldaOut <-LDA(dtm,k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
```
```{r}
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.topics
```
```{r}
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,10))
```

```{r topicsmodel Statistics}

#ldaOut <- LDA(speeches.matrix, k = 2, control = list(seed = 1234))
ap_topics <- tidy(ldaOut, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


```{r}
sessionInfo()
```

